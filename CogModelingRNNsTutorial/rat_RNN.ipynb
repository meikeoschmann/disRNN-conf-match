{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + defaults settings.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '.\\CogModelingRNNsTutorial')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    _ON_COLAB = True\n",
    "except:\n",
    "    _ON_COLAB = False\n",
    "\n",
    "import bandits\n",
    "import disrnn\n",
    "import hybrnn\n",
    "import plotting\n",
    "import rat_data\n",
    "import rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_array = np.load('..\\data\\miller2018_rat16.npy')\n",
    "\n",
    "dataset_train, dataset_test = rat_data.format_into_datasets(\n",
    "    *rat_data.load_data_for_one_rat(rat_array)[:2], rnn_utils.DatasetRNN)\n",
    "n_trials_per_session, n_sessions, _ = dataset_train._xs.shape\n",
    "experiment_list_train = None\n",
    "experiment_list_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of latent units in the model.\n",
    "latent_size = 5  #@param\n",
    "\n",
    "# Number of hidden units in each of the two layers of the update MLP.\n",
    "update_mlp_shape = (3,3,)  #@param\n",
    "\n",
    "# Number of hidden units in each of the two layers of the choice MLP.\n",
    "choice_mlp_shape = (2,)\n",
    "\n",
    "def make_disrnn():\n",
    "  model = disrnn.HkDisRNN(latent_size = latent_size,\n",
    "                          update_mlp_shape = update_mlp_shape,\n",
    "                          choice_mlp_shape = choice_mlp_shape,\n",
    "                          target_size=2)\n",
    "  return model\n",
    "\n",
    "def make_disrnn_eval():\n",
    "  model = disrnn.HkDisRNN(latent_size = latent_size,\n",
    "                          update_mlp_shape = update_mlp_shape,\n",
    "                          choice_mlp_shape = choice_mlp_shape,\n",
    "                          target_size=2,\n",
    "                          eval_mode=True)\n",
    "  return model\n",
    "\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: this step can be rather time consuming without GPU access. If you are not running on a GPU\n",
      ", you may want to set n_steps to a very low value and return to the exercise when you \n",
      "have access to hardware acceleration.\n",
      "Step 1000 of 1000; Loss: 5.6614e+03. (Time: 78.9s)"
     ]
    }
   ],
   "source": [
    "# You can experiment with different values, but colab has been tested with 1000.\n",
    "print(('Warning: this step can be rather time consuming without GPU access. If you are not running on a GPU\\n, '\n",
    "       'you may want to set n_steps to a very low value and return to the exercise when you \\n'\n",
    "       'have access to hardware acceleration.'))\n",
    "n_steps = 1000 #@param\n",
    "information_penalty = 0\n",
    "\n",
    "disrnn_params, opt_state, losses = rnn_utils.train_model(\n",
    "    model_fun = make_disrnn,\n",
    "    dataset = dataset_train, \n",
    "    optimizer = optimizer,\n",
    "    loss_fun = 'penalized_categorical',\n",
    "    penalty_scale=information_penalty,\n",
    "    n_steps=n_steps,\n",
    "    do_plot=False,\n",
    "    truncate_seq_length=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(dataset, model_fun, params):\n",
    "\n",
    "  xs, actual_choices = next(dataset)\n",
    "  n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
    "  model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
    "\n",
    "  predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
    "\n",
    "  log_likelihood = 0\n",
    "  n = 0  # Total number of trials across sessions.\n",
    "  for sess_i in range(n_sessions):\n",
    "    for trial_i in range(n_trials_per_session):\n",
    "      actual_choice = int(actual_choices[trial_i, sess_i])\n",
    "      if actual_choice >= 0:  # values < 0 are invalid trials which we ignore.\n",
    "        log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
    "        n += 1\n",
    "\n",
    "  normalized_likelihood = np.exp(log_likelihood / n)\n",
    "\n",
    "  print(f'Normalized Likelihood: {100 * normalized_likelihood:.1f}%')\n",
    "\n",
    "  return normalized_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Likelihoods for Disentangled RNN\n",
      "Training Dataset\n",
      "Normalized Likelihood: 60.8%\n",
      "Held-Out Dataset\n",
      "Normalized Likelihood: 59.2%\n"
     ]
    }
   ],
   "source": [
    "print('Normalized Likelihoods for Disentangled RNN')\n",
    "print('Training Dataset')\n",
    "training_likelihood = compute_log_likelihood(\n",
    "    dataset_train, make_disrnn_eval, disrnn_params)\n",
    "print('Held-Out Dataset')\n",
    "testing_likelihood = compute_log_likelihood(\n",
    "    dataset_test, make_disrnn_eval, disrnn_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
